{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e119f3ee-bdc9-42fb-a2f9-9c3a52aad5b1",
   "metadata": {},
   "source": [
    "# Preliminaries  \n",
    "\n",
    "**Overview**  \n",
    "This section provides the essential survival skills needed before diving into deep learning. It covers how to manipulate and preprocess data, the linear algebra and calculus concepts that underlie neural networks, the use of automatic differentiation, basic probability for reasoning under uncertainty, and how to effectively use documentation. These fundamentals ensure you can follow the technical content of later chapters with confidence.  \n",
    "<br>  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fab77-35fd-49b1-befa-3f18c8d2e68c",
   "metadata": {},
   "source": [
    "## A. Data Manipulation (2.1)\n",
    "\n",
    "**Recap**  \n",
    "This section introduces tensors, the core data structure in PyTorch. A tensor is just a container for numbers in one or more dimensions. We can create them, manipulate them with operations, automatically broadcast shapes when combining arrays of different sizes, save memory efficiently, and convert between PyTorch and other Python objects like NumPy.  \n",
    "\n",
    "**Vocab**  \n",
    "- **Tensor**: A general container for numbers in 1D (vector), 2D (matrix), or higher dimensions.  \n",
    "- **Broadcasting**: Expanding smaller arrays automatically to match larger shapes during operations.  \n",
    "\n",
    "**Notes**  \n",
    "- Tensors are arrays of numbers: 1D = vector, 2D = matrix, higher = tensor.  \n",
    "- Tensors support math operations (add, subtract, multiply, divide) applied elementwise.  \n",
    "- Broadcasting lets smaller tensors expand to match larger shapes automatically.  \n",
    "- In-place operations (`a.add_(b)`) save memory but overwrite values directly.  \n",
    "- Tensors can be converted to/from NumPy arrays or Python scalars, sharing memory in the process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93917ce0-64be-4859-a8f1-405e95051524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector v:\n",
      " tensor([0, 1, 2])\n",
      "\n",
      "Matrix M after in-place add with broadcasting:\n",
      " tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "\n",
      "Result (new tensor, not in-place):\n",
      " tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a vector and a matrix\n",
    "v = torch.arange(3)          # [0, 1, 2]\n",
    "M = torch.ones((3, 3))       # 3x3 of ones\n",
    "\n",
    "# Broadcasting: vector expands to match matrix shape\n",
    "result = M + v\n",
    "\n",
    "# In-place operation: overwrite to save memory\n",
    "M.add_(v)   # modifies M directly\n",
    "\n",
    "print(\"Original vector v:\\n\", v)\n",
    "print(\"\\nMatrix M after in-place add with broadcasting:\\n\", M)\n",
    "print(\"\\nResult (new tensor, not in-place):\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6029ffa-186d-4772-ae03-b494755aa6a7",
   "metadata": {},
   "source": [
    "## B. Data Preprocessing (2.2)\n",
    "\n",
    "**Recap**  \n",
    "This section shows how to prepare raw, messy data for machine learning. We load CSV files into pandas DataFrames, separate inputs from targets, fix missing values, encode categorical variables, and finally convert everything into PyTorch tensors for training.  \n",
    "\n",
    "**Vocab**  \n",
    "- Imputation: Replacing missing values with estimated ones, often using the mean.  \n",
    "- One-hot encoding: Expanding a categorical column into multiple binary indicator columns.  \n",
    "\n",
    "**Notes**  \n",
    "- Use `pd.read_csv()` to load tabular data into a DataFrame.  \n",
    "- Separate the dataset into inputs (features) and targets (labels).  \n",
    "- Missing numeric values are often imputed with the column mean.  \n",
    "- Missing categorical values can be handled by one-hot encoding, with NaN treated as its own category using `dummy_na=True`.  \n",
    "- Once data is fully numeric, convert to an array with `.to_numpy()` & then wrap it in a PyTorch tensor using `torch.tensor()`.  \n",
    "- Models require continuous float values because gradient-based optimization cannot work on integers or strings.  \n",
    "- Data preprocessing is essential because real-world datasets often contain missing a lot, and all must be cleaned before training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f1eb347-398b-4d44-952e-54ba8e9589b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATAFRAME\n",
      "   NumRooms RoofType   Price\n",
      "0       NaN     None  127500\n",
      "1       2.0     None  106000\n",
      "2       4.0    Slate  178100\n",
      "3       NaN     None  140000\n",
      "\n",
      ".dtypes:\n",
      " NumRooms    float64\n",
      "RoofType     object\n",
      "Price         int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " NumRooms    2\n",
      "RoofType    3\n",
      "Price       0\n",
      "dtype: int64\n",
      "\n",
      "--- AFTER SPLIT ---\n",
      "inputs:\n",
      "    NumRooms RoofType\n",
      "0       NaN     None\n",
      "1       2.0     None\n",
      "2       4.0    Slate\n",
      "3       NaN     None\n",
      "\n",
      ".targets (Series):\n",
      " 0    127500\n",
      "1    106000\n",
      "2    178100\n",
      "3    140000\n",
      "Name: Price, dtype: int64\n",
      "\n",
      "--- AFTER ONE-HOT ENCODING ---\n",
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       NaN           False          True\n",
      "1       2.0           False          True\n",
      "2       4.0            True         False\n",
      "3       NaN           False          True\n",
      "\n",
      "Columns now: ['NumRooms', 'RoofType_Slate', 'RoofType_nan']\n",
      "\n",
      ".dtypes after encoding:\n",
      " NumRooms          float64\n",
      "RoofType_Slate       bool\n",
      "RoofType_nan         bool\n",
      "dtype: object\n",
      "\n",
      "Missing values per column BEFORE fillna:\n",
      " NumRooms          2\n",
      "RoofType_Slate    0\n",
      "RoofType_nan      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column AFTER fillna:\n",
      " NumRooms          0\n",
      "RoofType_Slate    0\n",
      "RoofType_nan      0\n",
      "dtype: int64\n",
      "\n",
      "--- ENCODED + IMPUTED INPUTS ---\n",
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       3.0           False          True\n",
      "1       2.0           False          True\n",
      "2       4.0            True         False\n",
      "3       3.0           False          True\n",
      "\n",
      "--- TENSORS ---\n",
      "X:\n",
      " tensor([[3., 0., 1.],\n",
      "        [2., 0., 1.],\n",
      "        [4., 1., 0.],\n",
      "        [3., 0., 1.]])\n",
      "y:\n",
      " tensor([127500., 106000., 178100., 140000.])\n",
      "\n",
      "Shapes -> X: (4, 3)  y: (4,)\n",
      "Dtypes  -> X: torch.float32  y: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Raw data\n",
    "data = pd.DataFrame({\n",
    "    'NumRooms': [None, 2, 4, None],\n",
    "    'RoofType': [None, None, 'Slate', None],\n",
    "    'Price': [127500, 106000, 178100, 140000]\n",
    "})\n",
    "\n",
    "print(\"RAW DATAFRAME\")\n",
    "print(data)\n",
    "print(\"\\n.dtypes:\\n\", data.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", data.isna().sum())\n",
    "\n",
    "# Split inputs vs targets\n",
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "print(\"\\n--- AFTER SPLIT ---\")\n",
    "print(\"inputs:\\n\", inputs)\n",
    "print(\"\\n.targets (Series):\\n\", targets.head())\n",
    "\n",
    "\n",
    "# One-hot encode categorical (keep NaN as its own category)\n",
    "inputs_encoded = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(\"\\n--- AFTER ONE-HOT ENCODING ---\")\n",
    "print(inputs_encoded)\n",
    "print(\"\\nColumns now:\", list(inputs_encoded.columns))\n",
    "print(\"\\n.dtypes after encoding:\\n\", inputs_encoded.dtypes)\n",
    "\n",
    "\n",
    "# Impute numeric NaN with column means\n",
    "print(\"\\nMissing values per column BEFORE fillna:\\n\", inputs_encoded.isna().sum())\n",
    "inputs_filled = inputs_encoded.fillna(inputs_encoded.mean(numeric_only=True))\n",
    "print(\"\\nMissing values per column AFTER fillna:\\n\", inputs_filled.isna().sum())\n",
    "print(\"\\n--- ENCODED + IMPUTED INPUTS ---\")\n",
    "print(inputs_filled)\n",
    "\n",
    "# Convert to tensors (float)\n",
    "X = torch.tensor(inputs_filled.to_numpy(dtype=float), dtype=torch.float32)\n",
    "y = torch.tensor(targets.to_numpy(dtype=float), dtype=torch.float32)\n",
    "\n",
    "print(\"\\n--- TENSORS ---\")\n",
    "print(\"X:\\n\", X)\n",
    "print(\"y:\\n\", y)\n",
    "print(\"\\nShapes -> X:\", tuple(X.shape), \" y:\", tuple(y.shape))\n",
    "print(\"Dtypes  -> X:\", X.dtype, \" y:\", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b725a-9be0-480f-a2aa-a0dd5cabea67",
   "metadata": {},
   "source": [
    "## C. Linear Algebra (2.3)  \n",
    "\n",
    "**Recap**  \n",
    "Linear algebra is the language of deep learning. Vectors represent features or data points, matrices represent transformations, and norms measure size or distance. Operations like dot products, matrix multiplication, and solving systems form the math backbone of optimization and neural networks.  \n",
    "\n",
    "\n",
    "### Key Definitions  \n",
    "\n",
    "**Vector**  \n",
    "- A 1-D tensor with shape (n). Example: x = [x₁, x₂, …, xₙ].  \n",
    "- Represents a list of numbers such as weights, pixel values, or features.  \n",
    "\n",
    "**Matrix**  \n",
    "- A 2-D tensor with shape (m, n). Example: rows and columns of numbers.  \n",
    "- Represents a transformation that maps inputs in ℝⁿ to outputs in ℝᵐ.  \n",
    "\n",
    "**Transpose (Aᵀ)**  \n",
    "- Swaps rows ↔ columns. If A is (m×n), then Aᵀ is (n×m).  \n",
    "- Lets us align shapes for multiplication and is used in gradients.  \n",
    "\n",
    "**Dot Product (a·b = aᵀb)**  \n",
    "- a·b = ∑ᵢ aᵢbᵢ.  \n",
    "- Measures similarity: positive = same direction, 0 = perpendicular, negative = opposite.  \n",
    "\n",
    "**Matrix–Vector Product (Ax)**  \n",
    "- If A is (m×n) and x ∈ ℝⁿ, result is vector in ℝᵐ.  \n",
    "- Think: apply linear transformation A to the vector x.  \n",
    "\n",
    "**Matrix–Matrix Product (AB)**  \n",
    "- If A is (m×n), B is (n×p), result is (m×p).  \n",
    "- Represents composing two transformations: first B, then A.  \n",
    "\n",
    "**Norms**  \n",
    "- Measures the size or length of vectors or matrices.  \n",
    "- Vector norms:  \n",
    "  - L1: ‖x‖₁ = ∑ |xᵢ| (sum of absolute values).  \n",
    "  - L2: ‖x‖₂ = √(∑ xᵢ²) (Euclidean length).  \n",
    "  - L∞: ‖x‖∞ = max |xᵢ| (largest absolute entry).  \n",
    "- Matrix norm: Frobenius ‖A‖_F = √(∑ aᵢⱼ²), “length” of a matrix.  \n",
    "\n",
    "**Identity Matrix (I)**  \n",
    "- Diagonal of ones, zeros elsewhere.  \n",
    "- Acts like “1” in multiplication: Ix = x.  \n",
    "\n",
    "**Inverse (A⁻¹)**  \n",
    "- For square A, if invertible: A⁻¹A = I.  \n",
    "- Solves Ax = b as x = A⁻¹b (theoretical). In practice, use `solve()` for stability.  \n",
    "\n",
    "**Trace (tr(A))**  \n",
    "- Sum of diagonal elements.  \n",
    "- Shows up in derivatives and covariance formulas.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Identities & Theorems  \n",
    "\n",
    "**Cauchy–Schwarz Inequality**  \n",
    "Formula: |a·b| ≤ ‖a‖₂‖b‖₂  \n",
    "Meaning: The dot product can never be larger than the product of lengths.  \n",
    "Why it matters: Guarantees cosine similarity is always between -1 and 1.  \n",
    "\n",
    "**L2 Norm Identity**  \n",
    "Formula: ‖x‖₂² = xᵀx  \n",
    "Meaning: Squared length = dot product with itself.  \n",
    "Why it matters: Common trick for optimization, weight decay, and loss functions.  \n",
    "\n",
    "**Frobenius Norm Identity**  \n",
    "Formula: ‖A‖_F² = tr(AᵀA)  \n",
    "Meaning: Matrix length = sum of squared entries, also equals trace of AᵀA.  \n",
    "Why it matters: Used in regularization and proofs for gradients.  \n",
    "\n",
    "**Matrix Multiplication Associativity**  \n",
    "Formula: (AB)C = A(BC)  \n",
    "Meaning: Doesn’t matter how you group multiplications.  \n",
    "Why it matters: Lets us reorder computations for efficiency and simplifies derivations.  \n",
    "\n",
    "**Trace Cyclic Property**  \n",
    "Formula: tr(AB) = tr(BA)  \n",
    "Meaning: The order of multiplication inside a trace can be rotated.  \n",
    "Why it matters: Simplifies derivatives in backprop and proofs.  \n",
    "\n",
    "**Solving Linear Systems**  \n",
    "Formula: Ax = b → x = A⁻¹b (theory), but use `solve(A, b)` in practice.  \n",
    "Meaning: Finding x that satisfies the system of equations.  \n",
    "Why it matters: Neural nets are built on solving large systems efficiently.  \n",
    "\n",
    "---\n",
    "\n",
    "### Expanded Notes  \n",
    "\n",
    "- **Shapes rule everything**: Always track (rows, cols). Most errors in PyTorch come from mismatched dimensions.  \n",
    "- **Dot product intuition**: Big positive = aligned, zero = perpendicular, negative = opposite. Useful for similarity and projections.  \n",
    "- **Matrix–vector product**: Each row of A acts like a filter on x. In ML, this is exactly what happens inside a fully connected layer.  \n",
    "- **Matrix–matrix product**: Think of this as stacking multiple transformations. In deep learning, each layer’s weight matrix multiplies the previous layer’s output.  \n",
    "- **Norms**:  \n",
    "  - L1 encourages sparsity (used in Lasso).  \n",
    "  - L2 is smooth and common in optimization (weight decay).  \n",
    "  - L∞ is used for robustness, bounding maximum deviation.  \n",
    "  - Frobenius measures overall weight magnitude in a matrix.  \n",
    "- **Identity matrix**: Think of it as a “do nothing” transformation. Useful in defining inverses and in derivatives.  \n",
    "- **Inverse**: Rarely computed directly, because it’s slow and unstable. Always prefer solving systems with built-in solvers.  \n",
    "- **Trace**: Very handy in derivations — simplifies matrix calculus. E.g., gradient of tr(AX) w.r.t. X is Aᵀ.  \n",
    "- **Geometric picture**:  \n",
    "  - Vectors = arrows.  \n",
    "  - Norms = arrow lengths.  \n",
    "  - Dot = shadow/projection of one arrow on another.  \n",
    "  - Matrices = rotations, scalings, shears.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
