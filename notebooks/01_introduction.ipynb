{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c407be-32ef-4a7d-a795-fa1208d5cd1b",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "\n",
    "**Overview**  \n",
    "This section introduces the foundations of machine learning: how computers learn from data, the main problem types, and why deep learning became dominant. It explains key ideas like data, models, objectives, optimization, supervised and unsupervised learning, reinforcement learning, and the breakthroughs that made deep learning powerful. Below are my notes, with Pokémon examples to keep things fun and memorable.  \n",
    "<br> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa79d24-2aff-4ab3-a394-c6379b674e97",
   "metadata": {},
   "source": [
    "## A. Wake Word Example  \n",
    "\n",
    "*Recap:* Machine learning solves problems we can’t code by hand, like recognizing “Hey Siri.” Instead of writing exact rules, we build models that learn patterns from lots of examples.  \n",
    "\n",
    "**Vocab** \n",
    "- **Parameters**: Adjustable knobs that control how a model behaves.  \n",
    "- **Model family**: A collection of models defined by the same structure but with different parameter values.  \n",
    "- **Learning algorithm**: The process that tunes parameters using data.  \n",
    "\n",
    "\n",
    "**Notes**  \n",
    "- Everyday apps like Siri or Alexa use multiple ML models in seconds (speech recognition, intent detection, maps, prediction).  \n",
    "- Writing such programs from scratch is impossible because raw audio produces 44,000 samples per second.  \n",
    "- Humans recognize “Alexa” easily, but we cannot hard-code rules for it — that’s why we use machine learning.  \n",
    "- A model is like a program with adjustable knobs (parameters) that shape its behavior.  \n",
    "- A family of models is the set of all behaviors possible by turning those knobs.  \n",
    "- A learning algorithm uses data to set the knobs so the model performs well.  \n",
    "- Training = start with random parameters → feed data → adjust knobs → repeat until performance improves.  \n",
    "- Instead of programming exact rules, we “program with data,” teaching a model through examples (cats vs dogs).  \n",
    "- Deep learning is one powerful method among many, but all ML relies on learning patterns from data instead of coding rules.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732689f-e0d9-456c-965a-6f387913e77f",
   "metadata": {},
   "source": [
    "## B. Key Components  \n",
    "\n",
    "*Recap:* Every ML problem has the same ingredients: data, models, objectives, and optimization. Think of it like Pokémon battles — stats (data), the Pokédex (model), win/loss record (objective), and training to get stronger (optimization).  \n",
    "\n",
    "**Vocab**  \n",
    "- **Feature**: An input attribute, like a Pokémon’s Attack or Speed stat.  \n",
    "- **Label**: The target output, like predicting a Pokémon’s type.  \n",
    "- **Loss function**: A score that measures how wrong the model is.  \n",
    "\n",
    "**Notes**  \n",
    "\n",
    "1. **Data**  \n",
    "- Data is the foundation because models can only learn from what they see.  \n",
    "- Each example has features (like Pokémon stats) and a label (like species or type).  \n",
    "- Data must be numerical so models can process it mathematically.  \n",
    "- Some data is fixed-length (stat sheet), others variable-length (battle logs, text).  \n",
    "- More data reduces reliance on assumptions and enables more powerful models.  \n",
    "- Wrong, biased, or missing data leads to poor predictions (“garbage in, garbage out”).  \n",
    "\n",
    "2. **Models**  \n",
    "- A model maps inputs to outputs, like a Pokédex identifying Pokémon.  \n",
    "- Models differ in complexity: simple models solve simple tasks, deep models stack layers for complex tasks.  \n",
    "- Deep learning chains many transformations together, which is why it is “deep.”  \n",
    "\n",
    "3. **Objective Functions**  \n",
    "- Objective functions measure performance and tell us if the model is improving.  \n",
    "- Loss is usually defined so lower is better (e.g., squared error in regression).  \n",
    "- For classification, error rate or cross-entropy are common metrics.  \n",
    "- Training set performance ≠ real-world performance, so test sets are needed to check generalization.  \n",
    "- Overfitting happens when a model memorizes training examples but fails on new ones.  \n",
    "\n",
    "4. **Optimization Algorithms**  \n",
    "- Optimization algorithms adjust parameters to minimize loss.  \n",
    "- Gradient descent checks how small parameter changes affect loss and updates accordingly.  \n",
    "- Training is like a trainer refining battle strategy after each match, step by step.\n",
    "  \n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2d35f-6b78-4b38-afa6-c36a4266a723",
   "metadata": {},
   "source": [
    "## C. Kinds of Machine Learning Problems  \n",
    "\n",
    "*Recap:* Machine learning comes in many flavors. Some predict numbers, some predict categories, some tag multiple labels, others rank results, recommend items, learn sequences, or even act in environments.  \n",
    "\n",
    "**Vocab**  \n",
    "- **Regression**: Predicting numerical values, like house prices or Pokémon damage.  \n",
    "- **Classification**: Predicting categories, like whether a Pokémon is Fire, Water, or Grass.  \n",
    "- **Reinforcement learning**: Learning through trial and error with rewards.  \n",
    "\n",
    "**Notes**  \n",
    "1. **Supervised Learning**  \n",
    "- Uses labeled data (features + labels) to train models.  \n",
    "- The model learns the mapping from inputs to outputs, like training a Pokédex with labeled Pokémon examples.  \n",
    "- Most real-world ML applications are supervised.  \n",
    "\n",
    "2. **Regression**  \n",
    "- Predicts continuous values (“how much?”).  \n",
    "- Examples: house prices, rainfall, surgery time.  \n",
    "- In Pokémon: predicting damage output from stats and type matchups.  \n",
    "\n",
    "3. **Classification**  \n",
    "- Predicts categories (“which one?”).  \n",
    "- Binary classification has two classes (Legendary vs non-Legendary).  \n",
    "- Multiclass classification has many classes (Fire, Water, Grass).  \n",
    "- Models often output probabilities (e.g., 90% Fire-type).  \n",
    "- Hierarchical classification treats some mistakes as less severe (Pidgey vs Spearow is less bad than Pidgey vs Onix).  \n",
    "- Risk matters: even a small chance of disaster (poisonous mushroom) can outweigh accuracy.  \n",
    "\n",
    "4. **Tagging (Multi-label Classification)**  \n",
    "- Some inputs have multiple correct labels.  \n",
    "- In Pokémon: Charizard is Fire + Flying.  \n",
    "- Real-world: tagging blog posts or research papers with multiple categories.  \n",
    "\n",
    "5. **Search**  \n",
    "- Ranks results by relevance instead of just yes/no.  \n",
    "- Example: Google ranks web pages by query relevance.  \n",
    "- In Pokémon: ranking the strongest Fire-types when searched.  \n",
    "\n",
    "6. **Recommender Systems**  \n",
    "- Personalize suggestions for each user.  \n",
    "- Use explicit ratings or implicit behavior (clicks, skips).  \n",
    "- In Pokémon: suggesting a team based on your past battles.  \n",
    "- Feedback loops can bias results by reinforcing popularity.  \n",
    "\n",
    "7. **Sequence Learning**  \n",
    "- Handles data where order and context matter.  \n",
    "- Examples: translation, speech recognition, video understanding.  \n",
    "- In Pokémon: predicting battle outcomes turn by turn instead of just final stats.  \n",
    "\n",
    "8. **Unsupervised Learning**  \n",
    "- Finds structure in unlabeled data.  \n",
    "- Clustering groups similar items (sweepers vs tanks vs supports).  \n",
    "- Subspace methods reduce data to key dimensions (PCA).  \n",
    "- Embeddings capture relationships (Pikachu → Electric, Bulbasaur → Grass).  \n",
    "- Causality seeks underlying drivers of patterns.  \n",
    "- Generative models create new samples (new Pokémon designs).  \n",
    "\n",
    "9. **Self-Supervised Learning**  \n",
    "- Creates labels from the data itself.  \n",
    "- Text models predict missing words in sentences.  \n",
    "- Image models predict missing patches or arrangements.  \n",
    "- These models learn strong general-purpose representations for later tasks.  \n",
    "\n",
    "10. **Interacting with an Environment**  \n",
    "- Offline learning ignores feedback, but real agents affect their environment.  \n",
    "- Environments may change, remember, or resist.  \n",
    "- Distribution shift occurs when training data differs from future data.  \n",
    "\n",
    "11. **Reinforcement Learning**  \n",
    "- Trains agents to act for rewards.  \n",
    "- Agents observe, act, and get rewards in loops.  \n",
    "- In Pokémon: learning strategies through trial and error across many battles.  \n",
    "- Famous examples include AlphaGo and Atari game AIs.  \n",
    "- Key challenges: credit assignment (which action mattered), partial observability, and balancing exploration vs exploitation.  \n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5104f51-a081-4832-b0b6-b3851b9fce6c",
   "metadata": {},
   "source": [
    "## D. The Road to Deep Learning  \n",
    "\n",
    "*Recap:* Deep learning became possible because data exploded, GPUs got fast, and new techniques like attention and Transformers made models far more powerful. What used to take days now trains in minutes.  \n",
    "\n",
    "**Vocab**  \n",
    "- **Dropout**: A method that adds noise during training to prevent overfitting.  \n",
    "- **Attention**: A mechanism that lets models focus on important parts of input, like a trainer focusing on key moves.  \n",
    "- **Transformer**: A neural network architecture built entirely on attention, used in modern AI like ChatGPT.  \n",
    "\n",
    "**Notes**  \n",
    "- Growth of data, storage, and GPUs enabled modern deep learning.  \n",
    "- Classic models like neural nets became feasible with more compute.  \n",
    "- Dropout reduced overfitting by adding noise during training.  \n",
    "- Attention mechanisms and Transformers improved handling of sequences.  \n",
    "- Scaling up data, models, and compute consistently improved results.  \n",
    "- GANs generated realistic images; diffusion models surpassed them in flexibility.  \n",
    "- Training scaled to thousands of GPUs, cutting training time drastically.  \n",
    "- Open frameworks like PyTorch and TensorFlow made tools widely available.  \n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb4a3-30ae-4038-8416-3e0800cdb1fa",
   "metadata": {},
   "source": [
    "## E. Success Stories  \n",
    "\n",
    "*Recap:* Machine learning is everywhere — from reading checks, fraud detection, and search engines to personal assistants, medical AI, games like Go, and even self-driving cars.  \n",
    "\n",
    "**Vocab**  \n",
    "- **OCR (Optical Character Recognition)**: Technology that converts text in images into machine-readable form.  \n",
    "- **Bias**: Systematic errors in data or models that lead to unfair predictions.  \n",
    "\n",
    "**Notes**  \n",
    "- OCR has read mail and checks since the 1990s.  \n",
    "- Fraud detection powers payments for Visa, PayPal, and Stripe.  \n",
    "- AIs beat humans in chess, Go, and poker.  \n",
    "- Digital assistants understand and act on spoken commands.  \n",
    "- Image recognition error dropped from 28% to ~2% in 7 years.  \n",
    "- Medical AI matches experts in diagnoses like skin cancer.  \n",
    "- Self-driving cars use deep learning for perception tasks.  \n",
    "- AI impacts credit, hiring, and automation, raising fairness issues.  \n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904a1e5-7feb-4b5d-b2cd-7f54739ff730",
   "metadata": {},
   "source": [
    "## F. The Essence of Deep Learning  \n",
    "\n",
    "*Recap:* Deep learning replaces manual design with end-to-end models that learn everything from raw data. It works across many fields and thrives because we now have enough data and compute to make it possible.  \n",
    "\n",
    "**Vocab**  \n",
    "- **End-to-end training**: Training a full system directly from input to output, without hand-designed steps.  \n",
    "- **Feature engineering**: Manually designing input transformations before deep learning automated it.  \n",
    "\n",
    "**Notes**  \n",
    "- Deep learning uses many-layered neural networks trained end-to-end.  \n",
    "- It replaces manual feature engineering with learned filters.  \n",
    "- It handles raw data (pixels, audio, text) where shallow models fail.  \n",
    "- With abundant data, deep models adapt better than human-designed rules.  \n",
    "- Empirical trial-and-error drives much of the progress.  \n",
    "- Open-source tools and pretrained models accelerate innovation.  \n",
    "- Deep learning unifies vision, speech, and language tasks under one framework.  \n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb8f17-43a1-4124-8d45-dec0fd10354c",
   "metadata": {},
   "source": [
    "## G. Exercises  \n",
    "\n",
    "*Overview:* For this section, I am using **LinguaTrail**, an app I am currently developing that helps people learn languages based on their personal learning style. It generates an AI learning plan and adapts exercises dynamically so users learn in the way that works best for them.  \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Question 1**  \n",
    "Which parts of code that you are currently writing could be “learned”, i.e., improved by learning and automatically determining design choices that are made in your code? Does your code include heuristic design choices? What data might you need to learn the desired behavior?  \n",
    "\n",
    "**Response:**  \n",
    "In LinguaTrail, several current design choices are heuristic. For example, users select one of six fixed learning styles during onboarding, and the lesson flows are initially determined by these static defaults. While this works for a first-run experience, it assumes that the chosen style perfectly reflects the learner’s true preference, which is often not the case. Over time, the app could improve by learning from actual usage data—such as lesson completion rates, time spent on activities, patterns of mistakes, and preferred modalities—and adapting the learning plan dynamically. This reflects the chapter’s lesson that while heuristics are simple handcrafted rules, models trained on real data can adapt more effectively and improve with experience.  \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Question 2**  \n",
    "Which problems that you encounter have many examples for their solution, yet no specific way for automating them? These may be prime candidates for using deep learning.  \n",
    "\n",
    "**Response:**  \n",
    "In LinguaTrail, deep learning could be applied to several problems where many examples exist but no fixed rules are obvious. For instance, the app could adapt game mechanics and exercise sequences based on learner engagement, recommend exercises by analyzing past performance, or predict the difficulty of new content for each user. It could also cluster common learner mistakes, such as recurring verb tense errors, and propose targeted practice, or discover hidden learner archetypes that go beyond the six predefined styles. These challenges demonstrate the chapter’s key insight: when the problem has no clear rules but plenty of examples, deep learning is a strong candidate.  \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Question 3**  \n",
    "Describe the relationships between algorithms, data, and computation. How do characteristics of the data and the current available computational resources influence the appropriateness of various algorithms?  \n",
    "\n",
    "**Response:**  \n",
    "The relationship between algorithms, data, and computation is central to LinguaTrail’s design. The quantity and quality of learner data dictate which algorithms are appropriate. If the dataset is small or noisy, simpler models such as logistic regression or decision trees, paired with engineered features, are more reliable. However, with richer sequential data—like logs of lesson history, completion times, and accuracy—more advanced algorithms such as recurrent or transformer-based models can better capture learning patterns and adapt lesson sequences. Computational resources also constrain choices: lightweight models may be required for on-device adaptation in real time, while more complex models can be trained and deployed on servers where more compute is available. This aligns with the book’s lesson that the interplay of data, algorithms, and compute ultimately determines what is feasible.  \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Question 4**  \n",
    "Name some settings where end-to-end training is not currently the default approach but where it might be useful.  \n",
    "\n",
    "**Response:**  \n",
    "One clear example in LinguaTrail is notifications. Currently, the app defaults to having notifications “on” for all users, but this setting could be improved through end-to-end learning. By analyzing data such as past notification interactions, time of day, user streaks, and signs of fatigue (like skipped lessons), the system could directly learn the best times and contexts to send notifications. Similarly, lesson personalization could benefit from end-to-end approaches by mapping a learner’s history directly to the next best exercise, rather than chaining together multiple rule-based systems. This reflects the chapter’s key point that end-to-end training often outperforms piecemeal pipelines by jointly optimizing the entire process for the desired outcome.  \n",
    "\n",
    "&nbsp;\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
